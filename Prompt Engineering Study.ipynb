{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5245e8",
   "metadata": {},
   "source": [
    "https://www.promptingguide.ai/techniques/prompt_chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b270afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05f4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4761bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = read_txt(\"C:/Users/피엔케이피부임상연구센타(주)/Desktop/♣Prompt_Engineering/open_ai_personal_key.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee18c9e4",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8dc7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243466c6",
   "metadata": {},
   "source": [
    "# Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a9d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":role},\n",
    "            \n",
    "            #Few Shot starts here:\n",
    "            {\"role\":\"user\",\"content\":shot_one_input},\n",
    "            {\"role\":\"assistant\",\"content\":shot_one_output},\n",
    "            {\"role\":\"user\",\"content\":shot_two_input},\n",
    "            {\"role\":\"assistant\",\"content\":shot_two_output},\n",
    "            {\"role\":\"user\",\"content\":shot_three_input},\n",
    "            {\"role\":\"assistant\",\"content\":shot_three_output},\n",
    "            \n",
    "            #Finally send the data for which we want GPT to run the prompt and find solution\n",
    "            {\"role\":\"user\",\"content\":prompt}\n",
    "            ]\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3b869",
   "metadata": {},
   "source": [
    "# CoT (Chain of Thought)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae90db",
   "metadata": {},
   "source": [
    "### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b3e92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_cot(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a logical problem solver who always think a problem through step by step, one by one\"},\n",
    "\n",
    "            # Send the data for which we want GPT to find a solution\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"Let's think step by step.\"}\n",
    "#             {\"role\": \"assistant\", \"content\": \"Let's think not just step by step, but also one by one.\"}\n",
    "            ]\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e7110",
   "metadata": {},
   "source": [
    "### Few-shot CoT (Manual-CoT) \n",
    "- demonstration manually designed\n",
    "- better performance than zero-shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_one_input = '''\n",
    "Roger has 5 tennis balls. He buys 2 more cans of these tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does he have now?\n",
    "'''\n",
    "\n",
    "shot_one_output = '''\n",
    "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls.\n",
    "5 + 6 = 11. The answer is 11.'''\n",
    "\n",
    "prompt = '''\n",
    "A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue.\n",
    "How many blue golf balls are there?'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a22116",
   "metadata": {},
   "source": [
    "### Auto-CoT\n",
    "- Leverage LLM with \"Let's think step by step\" prompt\n",
    "- \"Let's think not just step by step, but also one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9c0f4",
   "metadata": {},
   "source": [
    "#### 1) Partition questions of a given dataset into a few clusters\n",
    "- valid when there are multiple questions (that could be clustered into a similar \"유형\" of problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d08b62",
   "metadata": {},
   "source": [
    "#### 2) Select a representative question from each cluster and generate its reasoning chain using zero-shot-CoT with simple heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71cfbd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_cot(prompt):#(cluster_one_question, cluster_two_question, prompt):\n",
    "    # cluster question sample\n",
    "    cluster_one_question = 'While shopping for music online, Zoe bought 3 country albums and 5 pop albums. Each album came with a lyric sheet and had 3 songs. How many songs did Zoe buy total?'\n",
    "    cluster_two_question = 'A chef needs to cook 9 potatoes. He has already cooked 7. If each potato takes 3 minutes to cook, how long will it take him to cook the rest?'\n",
    "    \n",
    "    # cluster sample generated answer\n",
    "    cluster_one_answer = zero_shot_cot(cluster_one_question)\n",
    "    cluster_two_answer = zero_shot_cot(cluster_two_question)\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\": \"You are a logical problem solver who always think a problem through step by step, one by one\"},\n",
    "            \n",
    "            # leveraging zero-shot-aot 1\n",
    "            {\"role\": \"user\", \"content\": cluster_one_question},\n",
    "            {\"role\": \"assistant\", \"content\": \"Let's think step by step.\"},\n",
    "            {\"role\":\"assistant\",\"content\":cluster_one_answer},\n",
    "            \n",
    "            # leveraging zero-shot-aot 2\n",
    "            {\"role\": \"user\", \"content\": cluster_two_question},\n",
    "            {\"role\": \"assistant\", \"content\": \"Let's think step by step.\"},\n",
    "            {\"role\":\"assistant\",\"content\":cluster_two_answer},\n",
    "            \n",
    "            #Finally send the data for which we want GPT to run the prompt and find solution\n",
    "            {\"role\":\"user\",\"content\":prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"Let's think step by step.\"}\n",
    "            ]\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a121fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step 1: Determine the total number of puppies which is 64.\\n\\nStep 2: Know how many puppies were sold. The store sold 28 puppies.\\n\\nStep 3: Subtract the number of sold puppies from the total. 64 - 28 = 36 puppies remain.\\n\\nStep 4: They put the remaining puppies into cages, with 4 in each cage. Now divide the remaining puppies by the number that can fit into each cage: 36 puppies ÷ 4 puppies/cage = 9 cages.\\n\\nSo, the pet store used 9 cages for the remaining puppies.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_auto_cot = 'The pet store had 64 puppies. They sold 28 of them and put the rest into cages with 4 in each cage. How many cages did they use?'\n",
    "auto_cot(prompt_auto_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87644b",
   "metadata": {},
   "source": [
    "# Self-Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da215fb",
   "metadata": {},
   "source": [
    "- Sample multiple, diverse reasoning paths through few-shot CoT, and use the generatinos to select the most consistent answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700a2747",
   "metadata": {},
   "source": [
    "# Generated Knowledge Prompting\n",
    "- Draw out higher confidence regarding the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18901749",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Part of golf is trying to get a higher point total than others. Yes or No?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5cf1b2",
   "metadata": {},
   "source": [
    "#### 1) Generate knowledge about subject (Auto-CoT & few-shot - general knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a6d51",
   "metadata": {},
   "source": [
    "Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "Input: A fish is capable of thinking.\n",
    "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2db406",
   "metadata": {},
   "source": [
    "# Prompt Chaining\n",
    "- Break tasks into subtasks.\n",
    "- LLM is prompted with a subtask and then its response is used as input to another prompt\n",
    "- LLM might struggle to address a very detailed prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0bc1f",
   "metadata": {},
   "source": [
    "### Example : given a question about a large document (ex: Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861786b6",
   "metadata": {},
   "source": [
    "#### 1) Extract relevant quotes to answer a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9dfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_one = '''\n",
    "You are a helpful assistant. Your task is to help answer a question given in a document. The first step is to extract quotes relevant to the question from the document, delimited by ####. Please output the list of quotes using <quotes></quotes>. Respond with \"No relevant quotes found!\" if no relevant quotes were found.\n",
    "####\n",
    "{{document}}\n",
    "####\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718ea31",
   "metadata": {},
   "source": [
    "#### 2) Include the outputs from prompt 1 to prompt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a7597",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_two = '''\n",
    "Given a set of relevant quotes (delimited by <quotes></quotes>) extracted from a document and the original document (delimited by ####), please compose an answer to the question. Ensure that the answer is accurate, has a friendly tone, and sounds helpful.\n",
    "####\n",
    "{{document}}\n",
    "####\n",
    "<quotes>\n",
    "- Chain-of-thought (CoT) prompting[27]\n",
    "- Generated knowledge prompting[37]\n",
    "- Least-to-most prompting[38]\n",
    "- Self-consistency decoding[39]\n",
    "- Complexity-based prompting[41]\n",
    "- Self-refine[42]\n",
    "- Tree-of-thought prompting[43]\n",
    "- Maieutic prompting[45]\n",
    "- Directional-stimulus prompting[46]\n",
    "- Textual inversion and embeddings[59]\n",
    "- Using gradient descent to search for prompts[61][62][63][64]\n",
    "- Prompt injection[65][66][67]\n",
    "</quotes>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625915b",
   "metadata": {},
   "source": [
    "'> side track (Top P, Top K)\n",
    "\n",
    "What is Top P?\n",
    "Top P refers to a sampling technique where the AI selects the next word in a sequence from the smallest set of most likely candidate words whose cumulative probability exceeds the threshold P. For instance, if P is set to 0.9, the model considers only the top words that together make up 90% of the probability mass and randomly selects one of them for the next word in the sequence.\n",
    "\n",
    "Effects of Adjusting Top P:\n",
    "\n",
    "Top P = 1: This setting considers all possible words for the next step in the generation, as long as they have any non-zero probability of occurring. This can potentially include more obscure or less likely word choices, leading to more deterministic and predictable responses, as it effectively becomes a form of \"Top K\" sampling with a very large K.\n",
    "Reducing Top P (e.g., to 0.9):\n",
    "Increased Creativity and Variety: By limiting the selection to a subset of the most probable words, the responses can become more creative and diverse. This prevents the model from always choosing the most obvious or common words, encouraging more varied and interesting outputs.\n",
    "Control Over Randomness: Lower values of Top P increase the randomness of the output. This can be useful for generating content that requires novelty or less predictable responses.\n",
    "Potential Reduction in Coherence: While a lower Top P can increase diversity, it might also lead to responses that are less coherent or slightly off-topic, as the model is more likely to include unusual word choices that fit less predictably into the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa9eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8396f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
